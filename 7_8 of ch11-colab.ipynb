{"cells":[{"cell_type":"markdown","metadata":{"id":"XsalpJ_KLSSv"},"source":["# 5교시 3.자연어 처리\n","- 언어의 규칙을 컴퓨터가 이해하기엔 너무 어려웠다.\n","- 딥러닝을 통해 단어를 2차원 평면에서 거대한 벡터공간에 좌표시키는 것이 가능. 단어 사이의 관계 파악이 가능했음.\n","- 확장하면, 단어 간의 확률을 하나의 벡터공간에 저장해놓은것이 언어모델이고, 챗지피티 같은 게 있음. 또 크게는 모든 언어에 대해서도 확장.\n","- 토큰화, 임베딩\n","- 토큰화: 커피, 한, 잔, 따로 -> 어디에서 끊어야 하는가\n","- 임베딩: 각 단어를 어느 수치를 주어야 벡터공간에서 적절한가? (신경망 이용)\n","- 모든 관계의 그룹, 관계를 알고 수정할 수 있게 됨\n","\n","- 토큰화 과정에서 Sequence로 만들어주어야 추후 패딩이 가능.\n","- texts_to_sequences로 텍스트를 시퀀스로 변환\n","sequences = tokenizer.texts_to_sequences(texts)\n","- 각기 다른 문장의 길이를 동일하게 맞추어 임베딩 하기 위해 패딩을 진행 (길이가 같아야 수정/변환이 가능하므로)"]},{"cell_type":"markdown","metadata":{"id":"dyGt_ra8LSSz"},"source":["## 1. 텍스트의 토큰화"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"BzzjnS8mpG-z","outputId":"39ecb4c0-8cbc-43b3-b4c2-77b541fa1b3c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1751935336823,"user_tz":-540,"elapsed":7887,"user":{"displayName":"jieun park","userId":"09627718888805364913"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","단어 인덱스:\n"," {'커피': 1, '한잔': 2, '어때': 3}\n"]}],"source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding\n","import numpy as np\n","\n","# 전처리할 텍스트를 정합니다.\n","text = '커피 한잔 어때'\n","\n","# Tokenizer 객체 생성 및 fit_on_texts로 단어 인덱스 학습\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts([text])\n","\n","# texts_to_sequences로 텍스트를 시퀀스로 변환\n","sequences = tokenizer.texts_to_sequences([text])\n","\n","# 단어 인덱스 확인\n","word_index = tokenizer.word_index\n","print(\"\\n단어 인덱스:\\n\", word_index)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jLYxeBsZpG-0","outputId":"2df83294-0d7f-4cfe-f562-ca90a618461a"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","단어 카운트:\n"," OrderedDict([('먼저', 1), ('텍스트의', 2), ('각', 1), ('단어를', 1), ('나누어', 1), ('토큰화합니다', 1), ('단어로', 1), ('토큰화해야', 1), ('딥러닝에서', 2), ('인식됩니다', 1), ('토큰화한', 1), ('결과는', 1), ('사용할', 1), ('수', 1), ('있습니다', 1)])\n","\n","문장 카운트:  3\n","\n","각 단어가 몇 개의 문장에 포함되어 있는가:\n"," defaultdict(<class 'int'>, {'단어를': 1, '토큰화합니다': 1, '텍스트의': 2, '각': 1, '먼저': 1, '나누어': 1, '단어로': 1, '토큰화해야': 1, '인식됩니다': 1, '딥러닝에서': 2, '있습니다': 1, '결과는': 1, '수': 1, '토큰화한': 1, '사용할': 1})\n","\n","각 단어에 매겨진 인덱스 값:\n"," {'텍스트의': 1, '딥러닝에서': 2, '먼저': 3, '각': 4, '단어를': 5, '나누어': 6, '토큰화합니다': 7, '단어로': 8, '토큰화해야': 9, '인식됩니다': 10, '토큰화한': 11, '결과는': 12, '사용할': 13, '수': 14, '있습니다': 15}\n"]}],"source":["# 단어 빈도수 세기\n","\n","# 전처리하려는 세 개의 문장을 정합니다.\n","docs = ['먼저 텍스트의 각 단어를 나누어 토큰화합니다.',\n","       '텍스트의 단어로 토큰화해야 딥러닝에서 인식됩니다.',\n","       '토큰화한 결과는 딥러닝에서 사용할 수 있습니다.',\n","       ]\n","\n","# 토큰화 함수를 이용해 전처리 하는 과정입니다.\n","tokenizer = Tokenizer()            # 토큰화 함수 지정\n","tokenizer.fit_on_texts(docs)       # 토큰화 함수에 문장 적용\n","\n","# 단어의 빈도수를 계산한 결과를 각 옵션에 맞추어 출력합니다.\n","print(\"\\n단어 카운트:\\n\", token.word_counts)\n","\n","# 출력되는 순서는 랜덤입니다.\n","print(\"\\n문장 카운트: \", token.document_count)\n","print(\"\\n각 단어가 몇 개의 문장에 포함되어 있는가:\\n\", token.word_docs)\n","print(\"\\n각 단어에 매겨진 인덱스 값:\\n\",  token.word_index)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"vZx1gajwpG-0","outputId":"fa52b31b-3e38-45de-d185-6a6c06c02b50","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1751935482501,"user_tz":-540,"elapsed":168,"user":{"displayName":"jieun park","userId":"09627718888805364913"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","단어 인덱스:\n"," {'내일': 1, '떡볶이': 2, '먹으러': 3, '가자': 4, '떡볶이는': 5, '참': 6, '맛있어': 7, '요즘': 8, '더워서': 9, '입맛이': 10, '없다': 11}\n","\n","시퀀스:\n"," [[1, 2, 3, 4], [5, 6, 7], [8, 9, 10, 11]]\n","\n","패딩된 시퀀스:\n"," [[ 1  2  3  4]\n"," [ 0  5  6  7]\n"," [ 8  9 10 11]]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step\n","\n","임베딩 결과:\n"," [[[ 0.02806461  0.01764183 -0.03729599 -0.03247909  0.00428285]\n","  [-0.04824338  0.00273188  0.00874938 -0.02217491 -0.04502351]\n","  [ 0.03482396 -0.02569324 -0.02845721  0.01500436  0.01807901]\n","  [-0.01886079  0.01083529  0.01428589 -0.03871486 -0.04510821]]\n","\n"," [[-0.03127792 -0.02976793 -0.0138091   0.03903208 -0.03142831]\n","  [ 0.03491836  0.03119615  0.03707058 -0.02153922  0.00623661]\n","  [-0.033401   -0.0337594   0.02087737  0.04334459 -0.02020617]\n","  [ 0.029542   -0.03242177 -0.04751403  0.04594194  0.04026134]]\n","\n"," [[ 0.04873325  0.03426032 -0.03514082  0.03783559  0.02488995]\n","  [-0.04748409 -0.04569974 -0.02566934  0.02626978 -0.02907143]\n","  [ 0.00656276 -0.00147523  0.04705269  0.02988234 -0.04544923]\n","  [-0.02258239  0.01140044  0.0393383   0.04465646  0.04231032]]]\n"]}],"source":["# 전처리할 텍스트를 정합니다.\n","texts =  ['내일 떡볶이 먹으러 가자', '떡볶이는 참 맛있어', '요즘 더워서 입맛이 없다']\n","\n","# Tokenizer 객체 생성 및 fit_on_texts로 단어 인덱스 학습\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(texts)\n","\n","# texts_to_sequences로 텍스트를 시퀀스로 변환\n","sequences = tokenizer.texts_to_sequences(texts)\n","\n","# 단어 인덱스 확인\n","word_index = tokenizer.word_index\n","print(\"\\n단어 인덱스:\\n\", word_index)\n","\n","##### 패딩을 통해 시퀀스 길이를 맞춥니다.\n","print(\"\\n시퀀스:\\n\", sequences)\n","padded_sequences = pad_sequences(sequences, 4)\n","print(\"\\n패딩된 시퀀스:\\n\", padded_sequences)\n","\n","model = Sequential()\n","model.add(Embedding(input_dim=len(word_index) + 1, output_dim=5, input_length=4))\n","#input_dim에 1을 더하는 것은 인덱스 0을 패딩 값으로 사용하기 위함.\n","#Keras의 Tokenizer는 단어 인덱스를 1부터 시작하기 때문에, 인덱스 0은 패딩 값으로 예약\n","#output_dim은 단어가 임베딩될 벡터의 길이\n","\n","# 임베딩 결과 확인\n","embedding_output = model.predict(padded_sequences)\n","print(\"\\n임베딩 결과:\\n\", embedding_output)\n"]}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/taehojo/fastcampus_ai/blob/master/colab/11-colab.ipynb","timestamp":1751921234732}],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.19"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}